include = ../../etc/env/environment_common.conf
include = ../../etc/env/es_connection.conf
include = ../../etc/env/hdfs_connection.conf
include = ../../etc/env/mysql_connection.conf
include = metrics.conf
include = features_enabled.conf

#Global Properties

debug.mode=true
local.mode=true
num.workers=1

#Standard 5-tuple fields

source.ip=ip_src_addr
source.port=ip_src_port
dest.ip=ip_dst_addr
dest.port=ip_dst_port
protocol=protocol

#Kafka Spout
spout.kafka.buffer.size.bytes=1024000
spout.kafka.consumer.id=pcap.kafka
spout.kafka.fetch.size.bytes=1024
spout.kafka.forcefromstart=false
spout.kafka.socket.timeout.ms=600000
spout.kafka.start.offset.time=-1
spout.kafka.zk.root=/storm/topology/pcap/kafka
spout.kafka.topic=pcap

#Parser Bolt
bolt.parser.enabled=true
bolt.parser.num.of.key.chars.to.use.for.shuffle.grouping=6
bolt.parser.ts.precision=MICRO

#Indexing Bolt
bolt.indexing.indexname=pcap_index
bolt.indexing.documentname=pcap_doc
bolt.indexing.bulk=200

#Alerts Indexing Bolt
bolt.alerts.indexing.indexname=alert
bolt.alerts.indexing.documentname=bro_alert
bolt.alerts.indexing.bulk=1

#Error Indexing Bolt
bolt.error.indexing.indexname=alert
bolt.error.indexing.documentname=bro_alert
bolt.error.indexing.bulk=1


#HDFS Bolt
bolt.hdfs.size.rotation.policy=5
bolt.hdfs.size.sink.policy=5
bolt.hdfs.fs.url=hdfs://nn1:8020

#Kafka Bolt
bolt.kafka.topic=pcap_enriched

#HBase Bolt
=true
bolt.hbase.table.name=pcap_test
## Define the hbase table columns in the form <cf1>:<cq11>,<cq12>,<cq13>|<cf2>:<cq21>,<cq22>|.......
bolt.hbase.table.fields=t:pcap
bolt.hbase.table.key.tuple.field.name=pcap_id
bolt.hbase.table.timestamp.tuple.field.name=timestamp
bolt.hbase.enable.batching=true
bolt.hbase.write.buffer.size.in.bytes=2000000
bolt.hbase.durability=SKIP_WAL
bolt.hbase.partitioner.region.info.refresh.interval.mins=60

#Error Indexing Bolt
bolt.error.indexing.indexname=error
bolt.error.indexing.documentname=lancope_error
bolt.error.indexing.bulk=1